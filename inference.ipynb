{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    class_names = sorted(os.listdir(\"/root/birdclef2024/inputs/train_audios/\"))\n",
    "    num_classes = len(class_names)\n",
    "    debug = False\n",
    " \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "\n",
    "    data_root = \"/root/birdclef2024/inputs/\"\n",
    "    train_path = \"/root/birdclef2024/inputs/train_audios/train_metadata.csv\"\n",
    "    test_path = '/root/birdclef2024/inputs/test_audios/unlabeled_soundscapes'\n",
    "    if len(glob(f'{test_path}/*.ogg'))==0:\n",
    "        test_path = '/kaggle/input/birdclef-2024/unlabeled_soundscapes'\n",
    "\n",
    "    SR = 32000\n",
    "    DURATION = 5\n",
    "    \n",
    "    infer_duration=10\n",
    "    \n",
    "    train_duration=10\n",
    "    \n",
    "    # Sed model\n",
    "#     model_ckpt = [\n",
    "#         '/kaggle/input/birdclef-openvino-comp/sed_v2s_final_30s_finetune/sed3_120.xml', #v2s\n",
    "#         '/kaggle/input/birdclef-openvino-comp/sed_se_half_ce/sed_se_120.xml', #seresnext26t\n",
    "#         '/kaggle/input/birdclef-openvino-comp/sed_b3ns_30s_finetune/sed3_b3ns_120.xml', #b3ns\n",
    "#     ]\n",
    "    \n",
    "    model_ckpt = [\n",
    "        # '/root/birdclef2024/outputs/sed_b3ns/openvino/exp002_rmv_dupfiles/sed_b3ns.xml'\n",
    "#         '/kaggle/input/birdclef2024-openvino-models/sed_v2s.xml', #v2s\n",
    "    ]    \n",
    "    \n",
    "    # CNN model\n",
    "    re_model_ckpt = [\n",
    "        '/root/birdclef2024/outputs/cnn_b3ns/openvino/exp002_rmv_dupfiles/cnn_b3ns.xml'\n",
    "        # '/kaggle/input/birdclef2024-openvino-models/cnn_b0ns.xml'\n",
    "#         '/kaggle/input/birdclef2024-openvino-models/cnn_v2s.xml'\n",
    "#         '/kaggle/input/birdclef-openvino-comp/openvino_models_comp_half/re_120.xml', #resnet34d\n",
    "#         '/kaggle/input/birdclef-openvino-comp/re_b3ns_ce/re_b3ns_120.xml', #b3ns\n",
    "#         '/kaggle/input/birdclef-openvino-comp/re_v2s_30s_finetune/re_v2s_120.xml', #v2s\n",
    "#         '/kaggle/input/birdclef-openvino-comp/re_b0ns_final/re_b0ns_120.xml', #b0ns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model = len(Config.model_ckpt)+len(Config.re_model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(Config.train_path)\n",
    "Config.num_classes = len(df_train.primary_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(df_test,num_workers=1,sleep=0,batch_size=1):\n",
    "    import openvino.runtime as ov\n",
    "    core = ov.Core()\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import os\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import torch.nn as nn\n",
    "    import timm\n",
    "    import librosa as lb\n",
    "    import soundfile as sf\n",
    "    from  soundfile import SoundFile \n",
    "    import torchaudio\n",
    "\n",
    "    import torch.nn as nn\n",
    "    import time\n",
    "    from torch.nn import functional as F\n",
    "    from torch.distributions import Beta\n",
    "    from torch.nn.parameter import Parameter\n",
    "    from joblib.externals.loky.backend.context import get_context\n",
    "    #torch.jit.enable_onednn_fusion(True)\n",
    "\n",
    "\n",
    "    class BirdDatasetSED(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self, df, sr = Config.SR,n_mels=128, fmin=0, fmax=None, step=None, res_type=\"kaiser_fast\",resample=True, duration = Config.DURATION, train = True):\n",
    "\n",
    "            self.df = df\n",
    "            self.sr = sr \n",
    "            self.n_mels = n_mels\n",
    "            self.fmin = fmin\n",
    "            self.fmax = fmax or self.sr//2\n",
    "\n",
    "            self.train = train\n",
    "            self.duration = duration\n",
    "\n",
    "            self.audio_length = self.duration*self.sr\n",
    "            self.step = step or self.audio_length\n",
    "\n",
    "            self.res_type = res_type\n",
    "            self.resample = resample   \n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def read_file(self, filepath):\n",
    "            #audio, orig_sr = torchaudio.load(filepath)\n",
    "            #if orig_sr != self.sr:\n",
    "            #    # sinc_interpolation\n",
    "            #    resample_transform = torchaudio.transforms.Resample(orig_sr, self.sr, resampling_method=\"kaiser_window\")\n",
    "            #    audio = resample_transform(audio)\n",
    "\n",
    "            audio, orig_sr = sf.read(filepath, dtype=\"float32\")\n",
    "\n",
    "            if self.resample and orig_sr != self.sr:\n",
    "                audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n",
    "\n",
    "            seconds = []\n",
    "            for i in range(self.audio_length, len(audio) + self.step, self.step):\n",
    "                start = max(0, i - self.audio_length)\n",
    "                end = start + self.audio_length\n",
    "                if end > len(audio):\n",
    "                    pass\n",
    "                else:\n",
    "                    seconds.append(int(end/self.sr))\n",
    "\n",
    "            # 端のデータをattentionかけれるようにデータを連結しておく\n",
    "            audio = np.concatenate([audio,audio,audio])\n",
    "            audios = []\n",
    "            for i,second in enumerate(seconds):\n",
    "                end_seconds = int(second)\n",
    "                start_seconds = int(end_seconds - Config.DURATION)\n",
    "\n",
    "                end_index = int(self.sr * (end_seconds + (Config.train_duration - Config.DURATION) / 2) ) + len(audio) // 3\n",
    "                start_index = int(self.sr * (start_seconds - (Config.train_duration - Config.DURATION) / 2) ) + len(audio) // 3\n",
    "                end_pad = int(self.sr * (Config.train_duration - Config.DURATION) / 2) \n",
    "                start_pad = int(self.sr * (Config.train_duration - Config.DURATION) / 2) \n",
    "                y = audio[start_index:end_index].astype(np.float32)\n",
    "                if i==0:\n",
    "                    y[:start_pad] = 0\n",
    "                elif i==(len(seconds)-1):\n",
    "                    y[-end_pad:] = 0\n",
    "                audios.append(y)\n",
    "            audios = np.stack(audios)\n",
    "            audios = torch.tensor(audios).float().unsqueeze(1)\n",
    "            spec384,spec256,spec300_another,spec_rev2s=transform_to_spec(audios,train=False)\n",
    "            return spec384,spec256,spec300_another,spec_rev2s\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return self.read_file(self.df.loc[idx, \"path\"])\n",
    "        \n",
    "    # 色んな時間解像度でmelspec_transformを作成\n",
    "    # 出力は1channel\n",
    "    hop_length384 = Config.infer_duration*Config.SR // (384-1)\n",
    "    melspec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=Config.SR, hop_length=hop_length384, n_mels=128, f_min=0, f_max=Config.SR//2, n_fft=2048, center=True, pad_mode='constant',norm='slaney',onesided=True,mel_scale='slaney')\n",
    "    hop_length256 = Config.infer_duration*Config.SR // (256-1)\n",
    "    melspec_transform256 = torchaudio.transforms.MelSpectrogram(sample_rate=Config.SR, hop_length=hop_length256, n_mels=128, f_min=0, f_max=Config.SR//2, n_fft=2048, center=True, pad_mode='constant',norm='slaney',onesided=True,mel_scale='slaney')\n",
    "    #hop_length224 = Config.infer_duration*Config.SR // (224-1)\n",
    "    #melspec_transform224 = torchaudio.transforms.MelSpectrogram(sample_rate=Config.SR, hop_length=hop_length224, n_mels=128, f_min=0, f_max=Config.SR//2, n_fft=2048, center=True, pad_mode='constant',norm='slaney',onesided=True,mel_scale='slaney')\n",
    "    hop_length300 = Config.infer_duration*Config.SR // (300-1)\n",
    "    melspec_transform300 = torchaudio.transforms.MelSpectrogram(sample_rate=Config.SR, hop_length=hop_length300, n_mels=128, f_min=50, f_max=14000, n_fft=1024, center=True, pad_mode='constant',norm='slaney',onesided=True,mel_scale='slaney')\n",
    "    melspec_transform_rev2s = torchaudio.transforms.MelSpectrogram(sample_rate=Config.SR, hop_length=320, n_mels=64, f_min=50, f_max=14000, n_fft=1024, center=True, pad_mode='constant',norm='slaney',onesided=True,mel_scale='slaney')\n",
    "    \n",
    "    db_transform = torchaudio.transforms.AmplitudeToDB(stype='power',top_db=80)\n",
    "\n",
    "    def transform_to_spec(audio,train=True):\n",
    "        import math\n",
    "        amin=1e-10\n",
    "        ref_value=1.0\n",
    "        db_multiplier = math.log10(max(amin, ref_value))\n",
    "        spec = melspec_transform(audio)     \n",
    "        #spec = torchaudio.functional.amplitude_to_DB(spec,multiplier=10,amin=amin,db_multiplier=db_multiplier,top_db=80)\n",
    "        spec = db_transform(spec)\n",
    "        spec256 = melspec_transform256(audio)\n",
    "        spec256 = db_transform(spec256)\n",
    "        \n",
    "        #spec224 = melspec_transform224(audio)\n",
    "        #spec224 = db_transform(spec224)\n",
    "        \n",
    "        spec300_another = melspec_transform300(audio)\n",
    "        spec300_another = db_transform(spec300_another)\n",
    "        \n",
    "        spec_rev2s = melspec_transform_rev2s(audio)\n",
    "        spec_rev2s = db_transform(spec_rev2s)\n",
    "        \n",
    "        spec384 = (spec+80)/80\n",
    "        spec256 = spec256/255\n",
    "        #spec224 = spec224/255\n",
    "        spec300_another = spec300_another/255\n",
    "        spec_rev2s = (spec_rev2s+80)/80\n",
    "        return spec384,spec256,spec300_another,spec_rev2s\n",
    "\n",
    "    \n",
    "    \n",
    "    def openvino_infer(model,data,tta):\n",
    "        outputs = model.infer(inputs=[data,tta])\n",
    "        outputs = torch.tensor(outputs[list(outputs.keys())[0]])\n",
    "        return outputs\n",
    "    \n",
    "    def openvino_infer_re(model,data):\n",
    "        outputs = model.infer(inputs=[data])\n",
    "        outputs = torch.tensor(outputs[list(outputs.keys())[0]])\n",
    "        return outputs\n",
    "    \n",
    "    def compute_deltas(\n",
    "            specgram: torch.Tensor,\n",
    "            win_length: int = 5,\n",
    "            mode: str = \"replicate\"\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Compute delta coefficients of a tensor, usually a spectrogram:\n",
    "\n",
    "        .. math::\n",
    "           d_t = \\frac{\\sum_{n=1}^{\\text{N}} n (c_{t+n} - c_{t-n})}{2 \\sum_{n=1}^{\\text{N}} n^2}\n",
    "\n",
    "        where :math:`d_t` is the deltas at time :math:`t`,\n",
    "        :math:`c_t` is the spectrogram coeffcients at time :math:`t`,\n",
    "        :math:`N` is ``(win_length-1)//2``.\n",
    "\n",
    "        Args:\n",
    "            specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "            win_length (int, optional): The window length used for computing delta (Default: ``5``)\n",
    "            mode (str, optional): Mode parameter passed to padding (Default: ``\"replicate\"``)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "\n",
    "        Example\n",
    "            >>> specgram = torch.randn(1, 40, 1000)\n",
    "            >>> delta = compute_deltas(specgram)\n",
    "            >>> delta2 = compute_deltas(delta)\n",
    "        \"\"\"\n",
    "        device = specgram.device\n",
    "        dtype = specgram.dtype\n",
    "\n",
    "        # pack batch\n",
    "        shape = specgram.size()\n",
    "        specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "        assert win_length >= 3\n",
    "\n",
    "        n = (win_length - 1) // 2\n",
    "\n",
    "        # twice sum of integer squared\n",
    "        denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "        specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "        kernel = torch.arange(-n, n + 1, 1, device=device, dtype=dtype).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "        output = torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "\n",
    "        # unpack batch\n",
    "        output = output.reshape(shape)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def make_delta(\n",
    "        input_tensor: torch.Tensor\n",
    "    ):\n",
    "        input_tensor = input_tensor.transpose(3,2)\n",
    "        input_tensor = compute_deltas(input_tensor)\n",
    "        input_tensor = input_tensor.transpose(3,2)\n",
    "        return input_tensor\n",
    "\n",
    "    \n",
    "    # 3channelに変更\n",
    "    def image_delta(x):\n",
    "        delta_1 = make_delta(x)\n",
    "        delta_2 = make_delta(delta_1)\n",
    "        x = torch.cat([x,delta_1,delta_2], dim=1)\n",
    "        return x\n",
    "    \n",
    "    def reshp(images):\n",
    "        # 4min*60/10inference duration=24batch size?\n",
    "        bs,clip_len,channel_num,mel_num,time_len = images.size()\n",
    "        images=images.reshape((bs*clip_len,channel_num,mel_num,time_len))\n",
    "        return images\n",
    "    \n",
    "    def predict(data_loader, models,re_models):   \n",
    "        predictions = []\n",
    "        pred_binary = []\n",
    "        dl_test = DataLoader(ds_test, batch_size=batch_size,num_workers = num_workers, multiprocessing_context=get_context('loky'))\n",
    "        \n",
    "        for spec384,spec256,spec300_another,spec_rev2s in dl_test:\n",
    "            spec384 = reshp(spec384)\n",
    "            spec256 = reshp(spec256)\n",
    "            spec300_another = reshp(spec300_another)\n",
    "            spec300_80 = (spec300_another*255+80)/80\n",
    "            spec_rev2s = reshp(spec_rev2s)\n",
    "            \n",
    "            out = []\n",
    "            for i,model in enumerate(models):\n",
    "                model_name = Config.model_ckpt[0].split('/')[-1].split('.')[0]\n",
    "                if model_name=='sed_v2s':\n",
    "                    images2_3chan = image_delta(spec384).numpy()\n",
    "\n",
    "                    if images2_3chan.shape[0]>120:\n",
    "                        output1 = openvino_infer(model,images2_3chan[:120,:,:,:],3)\n",
    "                        output2 = openvino_infer(model,images2_3chan[120:240,:,:,:],3)\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer(model,images2_3chan,3)\n",
    "                elif model_name=='sed_b3ns':\n",
    "                    images_3chan = image_delta(spec300_another).numpy()\n",
    "                    print(spec300_another.shape)\n",
    "                    if images_3chan.shape[0]>120:\n",
    "                        output1 = openvino_infer(model,images_3chan[:120,:,:,:],2)\n",
    "                        output2 = openvino_infer(model,images_3chan[120:240,:,:,:],2)\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer(model,images_3chan,2)\n",
    "                else:\n",
    "                    image_res = spec256.numpy()\n",
    "\n",
    "                    if image_res.shape[0]>120:\n",
    "                        output1 = openvino_infer(model,image_res[:120,:,:,:],2)\n",
    "                        output2 = openvino_infer(model,image_res[120:240,:,:,:],2)\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer(model,image_res,2)\n",
    "\n",
    "                out.append(outputs)\n",
    "            for i,model in enumerate(re_models):\n",
    "                model_name = Config.re_model_ckpt[0].split('/')[-1].split('.')[0]\n",
    "                if (model_name=='cnn_b0ns'):\n",
    "                    image_b0ns = spec256[:,:,:,:].numpy()\n",
    "                    if image_b0ns.shape[0]>120:\n",
    "                        output1 = openvino_infer_re(model,image_b0ns[:120,:,:,:])\n",
    "                        output2 = openvino_infer_re(model,image_b0ns[120:240,:,:,:])\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer_re(model,image_b0ns)\n",
    "                elif (i==1):\n",
    "                    images_center_resize2 = image_delta(spec300_80)[:,:,:,150:450].numpy()\n",
    "                    if images_center_resize2.shape[0]>120:\n",
    "                        output1 = openvino_infer_re(model,images_center_resize2[:120,:,:,:])\n",
    "                        output2 = openvino_infer_re(model,images_center_resize2[120:240,:,:,:])\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer_re(model,images_center_resize2)\n",
    "                elif (model_name=='cnn_v2s'):\n",
    "                    images_re_v2s = image_delta(spec_rev2s)[:,:,:,250:750].numpy()\n",
    "                    if images_re_v2s.shape[0]>120:\n",
    "                        output1 = openvino_infer_re(model,images_re_v2s[:120,:,:,:])\n",
    "                        output2 = openvino_infer_re(model,images_re_v2s[120:240,:,:,:])\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer_re(model,images_re_v2s)\n",
    "                elif (i==3):\n",
    "                    images_center_resize1 = image_delta(spec256)[:,:,:,128:384].numpy()\n",
    "                    if images_center_resize1.shape[0]>120:\n",
    "                        output1 = openvino_infer_re(model,images_center_resize1[:120,:,:,:])\n",
    "                        output2 = openvino_infer_re(model,images_center_resize1[120:240,:,:,:])\n",
    "                        outputs = torch.cat([output1,output2],dim=0)\n",
    "                    else:\n",
    "                        outputs = openvino_infer_re(model,images_center_resize1)\n",
    "                else:\n",
    "                    outputs = model(images_center_resize1)\n",
    "    \n",
    "                out.append(outputs)\n",
    "                \n",
    "            predictions.append(out)\n",
    "        return predictions\n",
    "\n",
    "    import gc\n",
    "\n",
    "    print(f\"Create Dataloader...\")\n",
    "\n",
    "    ds_test = BirdDatasetSED(\n",
    "        df_test, \n",
    "        sr = Config.SR,\n",
    "        duration = Config.DURATION,\n",
    "        train = False\n",
    "    )\n",
    "\n",
    "    \n",
    "    #print(\"Model Creation\")\n",
    "    models = []\n",
    "    for i,ckpt in enumerate(Config.model_ckpt):\n",
    "        #if i==0:\n",
    "        #    model = load_mdl(name,ckpt,size,sed_3chan=True)\n",
    "        #else:\n",
    "        #    model = load_mdl(name,ckpt,size)\n",
    "\n",
    "        model = core.read_model(model=ckpt)\n",
    "        model = core.compile_model(model, device_name=\"CPU\")\n",
    "        model = model.create_infer_request()\n",
    "        models.append(model)\n",
    "        \n",
    "    re_models = []\n",
    "    for i,ckpt in enumerate(Config.re_model_ckpt):\n",
    "\n",
    "        model = core.read_model(model=ckpt)\n",
    "        model = core.compile_model(model, device_name=\"CPU\")\n",
    "        model = model.create_infer_request()\n",
    "        re_models.append(model)\n",
    "\n",
    "    print(\"Running Inference..\")\n",
    "    time.sleep(sleep)\n",
    "    preds = predict(ds_test, models,re_models)   \n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8255</th>\n",
       "      <td>960500408</td>\n",
       "      <td>/root/birdclef2024/inputs/test_audios/unlabele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>2133074275</td>\n",
       "      <td>/root/birdclef2024/inputs/test_audios/unlabele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8024</th>\n",
       "      <td>906788264</td>\n",
       "      <td>/root/birdclef2024/inputs/test_audios/unlabele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>387939110</td>\n",
       "      <td>/root/birdclef2024/inputs/test_audios/unlabele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539</th>\n",
       "      <td>2051341574</td>\n",
       "      <td>/root/birdclef2024/inputs/test_audios/unlabele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                               path\n",
       "8255   960500408  /root/birdclef2024/inputs/test_audios/unlabele...\n",
       "4912  2133074275  /root/birdclef2024/inputs/test_audios/unlabele...\n",
       "8024   906788264  /root/birdclef2024/inputs/test_audios/unlabele...\n",
       "5737   387939110  /root/birdclef2024/inputs/test_audios/unlabele...\n",
       "4539  2051341574  /root/birdclef2024/inputs/test_audios/unlabele..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, path) for path in Path(Config.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"path\"]\n",
    ")\n",
    "# if not submission, use only 5 files out of unlabeled dataset\n",
    "if len(df_test)==8444:\n",
    "    df_test = df_test.sample(5)\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_num=2\n",
    "num_job = min([cpu_num,len(df_test)])\n",
    "split = len(df_test)//num_job\n",
    "num_job,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_test = []\n",
    "df_test_left = None\n",
    "for i in range(num_job):\n",
    "    df_test_split = df_test.iloc[i*split:(i+1)*split].reset_index(drop=True)\n",
    "    dfs_test.append(df_test_split)\n",
    "    if i==num_job-1:\n",
    "        df_test_left = df_test.iloc[(i+1)*split:].reset_index(drop=True)\n",
    "len(dfs_test),len(df_test_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Job 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataloader...\n",
      "Running Inference..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'images_center_resize3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,df_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dfs_test):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Job \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 347\u001b[0m, in \u001b[0;36mpred\u001b[0;34m(df_test, num_workers, sleep, batch_size)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Inference..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    346\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep)\n\u001b[0;32m--> 347\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mre_models\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "Cell \u001b[0;32mIn[18], line 305\u001b[0m, in \u001b[0;36mpred.<locals>.predict\u001b[0;34m(data_loader, models, re_models)\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m openvino_infer_re(model,images_center_resize1)\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[43mimages_center_resize3\u001b[49m)\n\u001b[1;32m    307\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(outputs)\n\u001b[1;32m    309\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images_center_resize3' is not defined"
     ]
    }
   ],
   "source": [
    "for i,df_test in enumerate(dfs_test):\n",
    "    print(f\"Running Job {i}\")\n",
    "    pred(df_test,2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataloader...\n",
      "Create Dataloader...\n",
      "Running Inference..\n",
      "Running Inference..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Check 'inputs.size() > idx' failed at src/inference/src/cpp/ie_infer_request.cpp:296:\nInput port for index 1 was not found! The model has only 1 inputs.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_16226/2450053763.py\", line 347, in pred\n  File \"/tmp/ipykernel_16226/2450053763.py\", line 267, in predict\n  File \"/tmp/ipykernel_16226/2450053763.py\", line 142, in openvino_infer\n  File \"/opt/conda/lib/python3.10/site-packages/openvino/runtime/ie_api.py\", line 152, in infer\n    return super().infer(normalize_inputs(self, {index: input for index, input in enumerate(inputs)}))\n  File \"/opt/conda/lib/python3.10/site-packages/openvino/runtime/ie_api.py\", line 100, in normalize_inputs\n    update_tensor(value, request, key)\n  File \"/opt/conda/lib/python3.10/functools.py\", line 889, in wrapper\n    return dispatch(args[0].__class__)(*args, **kw)\n  File \"/opt/conda/lib/python3.10/site-packages/openvino/runtime/ie_api.py\", line 78, in _\n    set_scalar_tensor(\n  File \"/opt/conda/lib/python3.10/site-packages/openvino/runtime/ie_api.py\", line 29, in set_scalar_tensor\n    request.set_input_tensor(key, tensor)\nRuntimeError: Check 'inputs.size() > idx' failed at src/inference/src/cpp/ie_infer_request.cpp:296:\nInput port for index 1 was not found! The model has only 1 inputs.\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m t1\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# delayedによってpred関数を遅延実行している\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#results1 = joblib.Parallel(n_jobs=num_job, backend='loky')(joblib.delayed(pred)(df_test) for df_test in dfs_test)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# sl引数は，time.sleep(sleep)で実行を遅らせて，CPU負荷を一気にかけるのを避けるため\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m results1 \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_job\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloky\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43msl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43msl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdfs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m t2\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(t2\u001b[38;5;241m-\u001b[39mt1)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Check 'inputs.size() > idx' failed at src/inference/src/cpp/ie_infer_request.cpp:296:\nInput port for index 1 was not found! The model has only 1 inputs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "# delayedによってpred関数を遅延実行している\n",
    "#results1 = joblib.Parallel(n_jobs=num_job, backend='loky')(joblib.delayed(pred)(df_test) for df_test in dfs_test)\n",
    "# sl引数は，time.sleep(sleep)で実行を遅らせて，CPU負荷を一気にかけるのを避けるため\n",
    "results1 = joblib.Parallel(n_jobs=num_job, backend='loky')(joblib.delayed(pred)(df_test,num_workers,sl,batch_size) for df_test,num_workers,sl,batch_size in zip(dfs_test,[2,2],[0,5],[1,1]))\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
